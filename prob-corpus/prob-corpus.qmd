---
title: "Training a classifier with a probabilistic data set:"
subtitle: "Discrete and weighted training with Bayes and maximum likelihood"
author: "Bob Carpenter"
date: "last-modified"
jupyter: python3
filters:
    - include-code-files
format:
  html:
    theme: cosmo
    css: style.css
    highlight-style: atom-one
    mainfont: Palatino
    fontcolor: black
    monobackgroundcolor: white
    monofont: "Menlo, Lucida Console, Liberation Mono, DejaVu Sans Mono, Bitstream Vera Sans Mono, Courier New, monospace"
    fontsize: 13pt
    linestretch: 1.4
    number-sections: true
    number-depth: 2
    toc: true
    toc-location: right
    cap-location: bottom
    format-links: false
    embed-resources: true
    anchor-sections: true
  pdf:
    include-in-header:
      - file: header.tex
    mainfont: Palatino
    number-sections: true
    number-depth: 2
    margin-bottom: 1in
    fig-pos: "t!"
    biblio-title: "References"
    biblio-style: natbib
    link-citations: true
    link-bibliography: true
    pdf-engine: xelatex
    highlight-style: github
bibliography: references.bib
---

#### Python libraries and logging {.unnumbered}

```{python}
#| code-fold: true
import numpy as np
import pandas as pd
import cmdstanpy as csp
import plotnine as pn

# don't wrap table output
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', None)

# drop MC standard error (col 1) and ESS/second (col 6)
def short_summary(fit):
   summary = fit.summary(percentiles=(5, 95), sig_figs=2)
   return summary.iloc[:, [0, 2, 3, 4, 5, 7]]
```

# Introduction

In this short technical note, I will consider several approaches to
training a binary classifier in the situation where the probability of
each item's outcome is known. The motivating example is the
probability assigned by a model of data rating (also known as coding,
annotation, labeling, and crowdsourcing) such as that of @dawid1979.
With a model of annotation, we only have noisy estimates of the true
probabilities, so I consider measurement error in the probability estimates.

The primary result I would like to highlight is that if you have
probabilistic information, it is best to use those probabilities to
train.  Best here means best at recovering true parameter values, and
hence best at prediction.  Training using probabilities means either
using logistic regression software that allows training with
probabilistically weighted outcomes or we can exploit the structure of
logistic regression and use the log odds of the probabilities to train
a linear regression.  The next best approach is to follow the
generative model and sample the outcomes given the probabilities and
train a non-probabilistic regression model using that data.  The worst
approach, by far, is to use the most probable outcome.  Approaches to
crowdsourcing using a majority vote approach are instances of this
suboptimal strategy.

I will evaluate these approaches to corpus construction and training
using both maximum likelihood and Bayesian posterior inference.  The
data is simulated so that we can benchmark error versus true parameter
values.


# Logistic regression

Logistic regression applies in the situation where there are $N$
binary observations $y_n \in \{ 0, 1 \}$ and each observation
comes with a (row) vector $x_n \in \mathbb{R}^D$ of covariates (also called
features or predictors).  Logistic regression is a generalized linear
model, with a parameter vector $\beta \in \mathbb{R}^D$ of
regression coefficients (also called weights), the link function is
log odds (also known as logit), and the family is binomial, so the
sampling distribution is
$$
Y_n \sim \textrm{binomial}(\textrm{logit}^{-1}(x_n \cdot \beta)),
$$
where $\textrm{logit}:(0, 1) \rightarrow \mathbb{R}$ and its inverse
$\textrm{logit}^{-1}:\mathbb{R} \rightarrow (0, 1)$ are defined by
$$
\textrm{logit}(u) = \log \frac{u}{1 - u}
\qquad
\textrm{logit}^{-1}(v) = \frac{1}{1 + \exp(-v)}.
$$
The definition of the binomial distribution implies
$$
\Pr[Y_n = 1] = \textrm{logit}^{-1}(x_n \cdot \beta).
$$

# Data simulation

For simplicity, we will simulate our parameter vector $\beta \in
\mathbb{R}^D$ as standard normal,
$$
\beta \sim \textrm{normal}(0, 1).
$$

Given a data size $N$, we generate a covariate matrix $x \in
\mathbb{R}^{N \times D}$ by taking
$$
x_n \sim \textrm{multi-normal}(0, \Sigma),
$$
where $\Sigma \in \mathbb{R}^{D \times D}$ is a full rank covariance
matrix (i.e., it is symmetric and positive definite).  We consider two
choices of covariance matrix.  First, we take the identity matrix $I$,
so that the covariates are generated as standard normal.  Second, we
will generate a covariance matrix corresponding to a first order
random walk, which will lead to highly correlated
predictors, as is often seen in practical applications in biomedicine,
social science, and machine learning.  The random-walk covariance
matrix is defined for a correlation value $\rho \in (-1, 1)$ by
$$
\Sigma_{i, j} = \rho^{| i - j |}.
$$
For example, with $D = 20$ and $\rho = 0.9$, the first row of the
covariance matrix is
$$
\Sigma_{1, 1:20} =
\begin{bmatrix}
1.00 & 0.90 & 0.81 & 0.73 & 0.66 & \cdots & 0.19 & 0.17 & 0.15 & 0.14
& 0.12
\end{bmatrix}.
$$

## Follow the data generating process

We can now simulate outcomes $Y_n$ in two different ways.  According
to the true data generating process, this should be done by sampling
$$
Y_n \sim \textrm{binomial}(\textrm{logit}^{-1}(x_n \cdot \beta).
$$

## Choose the most probable outcome

We can take the most probable outcome, by setting
A common approach in machine learning to deal with crowdsourcing with
multiple data coders is to take a majority vote or by the most
probable outcome in the probabilistic model.  This corresponds to
setting
$$
Y_n =
\begin{cases}
1 & \textrm{if } \Pr[Y_n = 1 \mid X_n = x_n] > \frac{1}{2}, \textrm{
and}
\\[4pt]
0 & \textnormal{otherwise}.
\end{cases}
$$
The name $Y_n$ is the same as in the previous section, but this is a
different random variable that is used as an alternative to the
previous definition (hence the same notation).

## Outcome probabilities

Rather than setting a single value for $Y_n$, we use the probability
estimates directly to train.  Here, we will assume the data consists
of outcome probabilities
$$
p_n = \Pr[Y_n = 1 \mid X_n = x_n].
$$
With direct probability estimates, we will be able to train using a
weighted logistic regression or with linear regression through a log
odds transform. 

# Priors, penalties, and objectives

## Bayesian prior

To complete our Bayesian model, which is a joint probability function
$p(y, \beta),$ we take independent standard normal priors for the
coefficients $d \in 1{:}D,$
$$
\beta_d \sim \textrm{normal}(0, 1).
$$
This prior matches our data generating process so that our model is
well specified for the data.

The full joint Bayesian model is defined by combining the prior and
sampling distributions, to give us the mixed continuous/discrete
probability function 
\begin{align}
p(y, \beta)
&= p(y \mid \beta) \cdot p(\beta).
\\[4pt]
&= \prod_{n=1}^N \textrm{bernoulli}(y_n \mid \textrm{logit}^{-1}(x_n \cdot
\beta))
\cdot \prod_{d=1}^D \textrm{normal}(\beta_d \mid 0, 1).
\end{align}

The objective for Bayesian estimation is the posterior distribution,
which has the following conditional density function (up to a proportion),
$$
p(\beta \mid y) \propto p(y \mid \beta) \cdot p(\beta).
$$
We use the notation $g$ to indicate that it is an unnormalized density.
We integrate over the objective function to perform Bayesian inference.

## Frequentist penalty function

From the frequentist perspective, we have a log likelihood function for
the data $(x, y)$ defined as a function of parameters $\beta,$
\begin{align}
\mathcal{L}(\beta)
&= \log \left( \prod_{n=1}^N \textrm{bernoulli}(y_n \mid \textrm{logit}^{-1}(x_n
\cdot \beta)) \right)
\\[4pt]
&= \sum_{n=1}^N \log \textrm{bernoulli}(y_n \mid \textrm{logit}^{-1}(x_n \cdot \beta)).
\end{align}

We will evaluate penalized maximum likelihood, where we will use the
penalty function
$$
\mathcal{P}(\beta) = \frac{1}{2} \beta^\top \beta.
$$
This penalty is called an L2 penalty because it's based on the (half
of the squared) L2-norm of $\beta$.  Using an L2 penalty function for
penalized maximum likelihood estimation is known as ridge regression.
The L2 penalty shrinks estimates toward zero by penalizing the
components of $\beta$ quadratically.

The objective function for frequentist estimation is known as the
penalized maximum likelihood function and is defined by
$$
\mathcal{F}(\beta) = \mathcal{L}(\beta) - \mathcal{P}(\beta).
$$
Frequentist estimation proceeds by optimization over the objective.

## Frequentist and Bayesian objectives are the same

The frequentist objective function is just the log posterior plus a
constant,
$$
\mathcal{F}(\beta) = \log p(\beta \mid y) + \textrm{const}.
$$
The constant, which we will not need to carry out Bayesian inference,
is $\log p(y).$

This relation does not imply that the Bayesian and frequentist
estimates will be the same.  Full Bayesian inference will average
over uncertainty rather than optimize.

# Estimation

For each observation $n \in 1{:}N,$ we have the covariate vector $x_n
\in \mathbb{R}^D$.

## Estimation from training data

The first two cases of simulation, sampling from the data generating
process and assigning the most probable category involve data $y_n \in
\{ 0, 1 \}$ for $n \in 1{:}N.$

### Bayesian estimate from training data

Given a data set $(x, y),$ the Bayesian parameter estimate is the posterior mean,
\begin{align}
\widehat{\beta}
&= \mathbb{E}[\beta \mid x, y]
\\[4pt]
&= \int_{\mathbb{R}^D} \beta \cdot p(\beta \mid x, y) \, \textrm{d}\beta.
\end{align}
We can use Markov chain Monte Carlo methods to take a sample
$$
\beta^{(1)}, \ldots, \beta^{(M)} \sim p(\beta \mid x, y),
$$
and estimate
$$
\widehat{\beta}
\approx
\frac{1}{M} \sum_{m=1}^M \beta^{(m)}.
$$
As $M \rightarrow \infty,$ the Monte Carlo approximation becomes exact
in the limit.  With $M = 10,000,$ we usually have around 2 digits of accuracy.


### Frequentist estimate from data

The frequentist estimate is the penalized maximum likelihood estimate,
$$
\beta^* = \textrm{arg max}_\beta \ \textrm{F}(\beta).
$$
Because of the relation between the Bayesian and frequentist
objectives for this problem, this estimate is also called the maximum
a posterior estimate because
$$
\beta^* = \textrm{arg max}_\beta \ p(\beta \mid x, y)
$$

## Probability-weighted estimation

In probability weighted training, we modify the objective function so
the data consists of weights $p_n \in (0, 1)$ for outcomes $n \in
1{:}N.$

### Bayesian objective

The Bayesian objective is defined by weighting the outcomes probabilistically,
$$
p(\beta \mid x, p)
\propto p(\beta)
        \cdot \prod_{n=1}^N
	       \left( \strut p(Y_n = 1 \mid x_n, \beta)^{p_n}
                      \cdot p(Y_n = 0 \mid x_n, \beta)^{1 - p_n} \right),
$$
which on the log scale is
$$
\log p(\beta \mid x, p)
= \log p(\beta)
  + \sum_{n=1}^N \left( \strut p_n \log p(Y_n = 1 \mid x_n, \beta)
                        + (1 - p_n) \log p(Y_n = 0 \mid x_n, \beta) \right)
  + \textrm{const}.
$$

This objective is known as the Rao-Blackwellized form of our original
objective.  The Rao-Blackwell theorem entails that working in
expectation, as we are doing here, perfroms no worse than sampling in
terms of estimation error; in practice, Rao-Blackwellizing usually
brings substantial gains, especially in the context of discrete
sampling as we are doing here.

For estimation, we use posterior means as before, swapping in this
objective for the previous one.

### Frequentist objective

The frequentist objective here mirrors the Bayesian objective,
$$
\mathcal{F}(\beta)
= + \sum_{n=1}^N \left( \strut p_n \log p(Y_n = 1 \mid x_n, \beta)
                        + (1 - p_n) \log p(Y_n = 0 \mid x_n, \beta)
			\right)
  - \frac{1}{2}\beta^2.
$$  
For estimation, we optimize this objective.


## Regression on the log odds

The final estimation technique involves recognizing that when working
in expectation, the log odds link function can be used to transform
the probabilistic data to the log odds scale, at which point
estimation can proceed via linear regression.  Because the location
and scale parameters in a linear regression are conditionally
independent given the data, we just fix the scale parameter to unity.

### Bayesian regression on log odds

The Bayesian objective function is defined by the data generating
process
$$
\textrm{logit}(p_n) \sim \textrm{normal}(x_n \cdot \beta, 1),
$$
which leads to the objective function
$$
p(\beta \mid x, p)
\propto \left( \prod_{d=1}^D \textrm{normal}(\beta_d \mid 0, 1) \right)
  \cdot \prod_{n=1}^N \textrm{normal}(\textrm{logit}(p_n) \mid x_n \cdot \beta, 1).
$$
On the log scale, that's
$$
\log p(\beta \mid x, p)
= \left( \sum_{d=1}^D \log \textrm{normal}(\beta_d \mid 0, 1) \right)
  + \sum_{n = 1}^N \log \textrm{normal}(\textrm{logit}(p_n) \mid x_n
  \cdot \beta, 1)
  + \textrm{const}.
$$  

For estimation, we use posterior means as before, swapping in this
objective for the previous one.

### Frequentist regression on log odds

The frequentist objective function is the same up to a constant,
$$
\mathcal{F}(\beta)
=  \left( \sum_{n=1}^N \log \textrm{normal}(\textrm{logit}(p_n) \mid
x_n \cdot \beta, 1) \right)
- \frac{1}{2} \beta^\top \cdot \beta.
$$
For estimation, we use optimziation on the objective function.

# Stan models

There are three objective functions in play, based on whether the data
under consideration is given as discrete outcomes $y_n$ or
probabilities $p_n$, and in the case of probabilities, whether they
are transformed to log odds for a linear regression.  

## Stan program for logistic regression

The Stan program for logistic regression implements the following log
posterior up to an additive constant, 
$$
\log p(\beta \mid x, y)
= \log p(\beta)
  + \sum_{n=1}^N \textrm{bernoulli}(\textrm{logit}^{-1}(x_n \cdot
  \beta)).
$$  

```{.stan include="logistic-regression.stan"
          filename="logistic-regression.stan"}
```


## Stan program for probability-weighted logistic regression

The Stan program for probability-weighted logistic regression
implements the following log posterior up to an additive constant,
$$
\log p(\beta \mid x, p)
= \log p(\beta)
  + \sum_{n=1}^N \left( \strut
      p_n \cdot \log \textrm{bernoulli}(1 \mid \textrm{logit}^{-1}(x_n \cdot \beta))
      + (1 - p_n) \cdot \textrm{bernoulli}(0 \mid \textrm{logit}^{-1}(x_n \cdot \beta))
    \right)     
  + \textrm{const}.
$$

```{.stan include="weighted-logistic-regression.stan"
          filename="weighted-logistic-regression.stan"}
```

## Stan program for log odds linear regression

The Stan program for linear regression on the log odds implements the
following log posteiror up to an additive constant.
$$
\log p(\beta \mid x, p)
= \log p(\beta)
  + \sum_{n = 1}^N
    \log \textrm{normal}(\textrm{logit}(p_n) \mid x_n \cdot \beta, 1)
  + \textrm{const}.
$$



```{.stan include="log-odds-linear-regression.stan"
          filename="log-odds-linear-regression.stan"}
```


