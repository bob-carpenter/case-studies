---
title: "Combining rating and ranking:"
subtitle: "Plackett-Luce ranking and ordinal logit rating models in Stan"
author: "Bob Carpenter"
date: "last-modified"
jupyter: python3
filters:
    - include-code-files
format:
  html:
    theme: cosmo
    css: style.css
    highlight-style: atom-one
    mainfont: Palatino
    fontcolor: black
    monobackgroundcolor: white
    monofont: "Menlo, Lucida Console, Liberation Mono, DejaVu Sans Mono, Bitstream Vera Sans Mono, Courier New, monospace"
    fontsize: 13pt
    linestretch: 1.4
    number-sections: true
    number-depth: 2
    toc: true
    toc-location: right
    cap-location: bottom
    format-links: false
    embed-resources: true
    anchor-sections: true
  pdf:
    include-in-header:
      - file: header.tex
    mainfont: Palatino
    number-sections: true
    number-depth: 2
    margin-bottom: 1in
    fig-pos: "t!"
    biblio-title: "References"
    biblio-style: natbib
    link-citations: true
    link-bibliography: true
    pdf-engine: xelatex
    highlight-style: github
bibliography: references.bib
---

# Preface {.unnumbered}

## Python setup {.unnumbered}

We will first load all of the libraries in Python that we will need.
The first batch is just to download the data.

```{python}
import urllib.request
import zipfile
import os
```

The real work is being done by these packages for simulation and
plotting. 

```{python}
import scipy as sp
import numpy as np
import pandas as pd
import cmdstanpy as csp
import plotnine as pn
```


# Introduction

In this case study, we will develop two standard models of preference
data, the Plackett-Luce model of ranking and the ordinal logistic
model of rating.  We will further combine them into a single model
with a likelihood for each data stream.

# The data

Toshihiro Kamishima collected data from $R = 5000$ Japanese raters about
their preferences for sushi and made it available:

* [Sushi Preference data set](https://www.kamishima.net/sushi/)

For each of the $R$ raters, they collected preference data for $K = 10$
types of sushi selected at random from a total collection of $I = 100$ different
types of sushi.  Each rater provided two forms of rating,

1. rank ordering of 10 pieces of sushi
2. ordinal ratings on a 1--5 scale of the pieces of sushi (5 being better)

## Ranking data

For the rank ordering, the head of the rank ordering data file looks like this:

`sushi3-2016/sushi3b.5000.10.order`
```
100 1
0 10 58 4 3 44 87 60 67 1 12 74
0 10 22 31 60 21 8 24 6 12 17 76
0 10 2 15 13 1 6 25 46 74 56 55
0 10 8 0 3 9 24 43 4 5 29 40
...4996 more lines...
```

The first row is just metadata about how many pieces of sushi there
are.  The first column is just a meaningless 0 and the second an
indicator of how many items are ranked.  The remaining columns say
that the first rater ranked type 58 higher than type 4, which was in
turn higher than type 3, and so on.

For more details on how the data was collected, see the readme with
the data distribution or the paper by @kamishima2003.

## Rating data

For the rating data, there is one row per rater.  A rating of -1 indicates missing data---items not rated.

`sushi3-2016/sushi3b.5000.10.score`
```
-1 0 -1 4 2 -1 -1 -1 -1 -1 -1 -1 1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 4 -1 2 -1 -1 -1 -1 -1 -1 0 -1 -1 -1 -1 -1 -1 0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 2 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
-1 -1 -1 -1 -1 -1 0 -1 1 -1 -1 -1 0 -1 -1 -1 -1 0 -1 -1 -1 1 2 -1 0 -1 -1 -1 -1 -1 -1 1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
...4996 more lines...
```

## Downloading the data {.unnumbered}

We do not have permission to redistribute the data with our case
study, so we first download it and unzip it into the directory
`sushi3-2016`.

```{python}
#| code-fold: true
url = "https://www.kamishima.net/asset/sushi3-2016.zip"
download_path = "sushi3-2016.zip"
output_dir = "./"
if not os.path.isdir("sushi3-2016"):
    urllib.request.urlretrieve(url, download_path)
    with zipfile.ZipFile(download_path, 'r') as zip_ref:
        zip_ref.extractall(output_dir)
```


# The Bradley-Terry model

@bradley1952 collected data from consumers about their preferences
between pairs of consumer goods among a larger set of such goods.

## Preference probability

The model works by assigning a latent quality score to each
item and using a logistic model of the probability of a consumer
preferring one to another.  Technically, we assume we have $I \in
\mathbb{N}$ such items and they have scores $\alpha_i \in \mathbb{R}.$
We let $Y \in \{ 0, 1 \}$ be the binary random variable taking on
value 1 if a specific rater $r$ ranks item $i$ higher than item $j$.
This probability is modeled as 
$$
\Pr[\textrm{rater } r \textrm{ prefers } i \textrm{ to } j]
= \textrm{logit}^{-1}(\alpha_i - \alpha_j),
$$
where the log odds function $\textrm{logit}:(0, 1) \rightarrow \mathbb{R}$ is defined by
$$
\textrm{logit}(u) = \log\left( \frac{u}{1 - u} \right).
$$
It's inverse is the logistic function $\textrm{logit}^{-1}:\mathbb{R}
\rightarrow (0, 1)$, defined by 
$$
\textrm{logit}^{-1}(v) = \frac{\exp(v)}{1 + \exp(v)}.
$$
A little algebra shows that
$$
\textrm{logit}^{-1}(\alpha_i - \alpha_j)
= \frac{\exp(\alpha_i)}{\exp(\alpha_i) + \exp(\alpha_j)}.
$$
In other words, $\alpha_i$ is proportional to the log odds.


## Bradley-Terry sampling distribution

Because not every rater is going to rate every item, we have what is
known as an incomplete block design.  To accomodate that notationally,
we will assume that we collect a total of $N$ ratings and that rating
$n \in 1{:}N$ is between items $i_n$ and $j_n$.  Putting our model into
generative form, we have

$$
Y_n \sim \textrm{bernoulli}(\textrm{logit}^{-1}(\alpha_{i_n} -
\alpha_{j_n})).
$$

## Non-identifiability of full-rank model

If we let the $\alpha_i$ be independent parameters, the model is not
identified because
$$
p(y \mid \alpha) = p(y \mid \alpha + c)
$$
for any constant $c \in \mathbb{R}$.  This form of additive
non-identifiability can be mitigated in several ways.

The first approach to identifying the model is to fix (aka pin, clamp)
one of the parameters to a fixed value, conventionally $\alpha_1 = 0$.
This method is not ideal because it makes the model asymmetric in the
parameters, which presents a challenge to assigning reasonable priors
under exchangeability assumptions (i.e., the ordering of the
items shouldn't matter).

Second, the set of parameters can be constrained to sum to a fixed
value, conventionally $\sum_{i = 1}^I \alpha_i = 0$.  This can be done
by taking $I - 1$ free paramters and setting $\alpha_I = -\sum_{i =
1}^{I-1} \alpha_i$.

With either of these solutions, the model can be fit with maximum
likelihodo if the data are not separable.  

A typical example of separability would arise if
there is one item $i$ that everyone prefers---in that case, there is no
maximum likelihood estimate as the likleihood increases as $\alpha_i
\rightarrow \finty$).

To deal with separability, we can take a third approach to identifying
the combined prior and likelihood, which is to add a proper Bayesian
prior such as $\alpha_i \sim \textrm{normal}(0, \sigma)$.  The value
of $\sigma$ may be fixed or it may be taken as a hyperparameter and
modeled hierarchically.  The second approach (sum to zero) and the
third approach (proper Bayesian prior) can be fruitfully combined.

## Positive parameterization

An alternative way to parameterize the Bradley-Terry model is to use
all-positive parameters $\beta_i > 0$ and set
$$
\Pr[\textrm{rater } r \textrm{ prefers } i \textrm{ to } j]
= \frac{\beta_i}{\beta_i + \beta_j}.
$$
Under this parameterization, the model has a multiplicative
non-identifiability in that
$$
p(y \mid \beta) = p(y \mid c \cdot \beta)
$$
for any $c \in (0, \infty)$.

Multiplicative non-identifiabilities can be mitigated in roughly the
same three ways as additive non-identifiabilities.  First, a value can
be pinned to a positive value such as 1.  Second, the values can be
constrained to have a fixed product, such as 1, by constraining $I -
1$ parameters to be positive and setting $\beta_I = \left(
\prod_{i=1}^{I - 1} \beta_i \right)^{-1}$.  Third, we can use a proper
and positively constrained distribution to define a prior.

There is a simple alternative approach that identifies the model and
handles the multiplicative non-identifiability at the same time.  We
can simply let the parameters form a simplex.  Specifically, this
means constraining $\beta$ by requiring $\beta_i > 0$ and
$\sum_{i=1}^I \beta_i = 1$.  Tne sum-to-one constraint sets the scale
and the simplex has $I - 1$ dimensions (because $\beta_I = 1 - \sum_{i
= 1}^{I - 1} \beta_i$).

## Relation to Elo

The famous chess ranking model of @elo1967 was discovered
independently, but it is just a reparameterization of the
Bradley-Terry model.  Elo used a
scaled, base-10 sigmoid function
$$'
f(u) = \frac{1}{1 + 10^{-u / 400}}
$$
rather than the logistic sigmoid ($\textrm{logit}^{-1}$).  A simple
change of variables turns it back into the Bradley-Terry model.
Presumably Elo's intent was to make it easier for non-mathematicians
to understand and to put the parameter values on a ``human'' scale
(i.e., order 1000)

@elo1967 also introduced an update rule, which allows rankings to be
updated after observing a single match.  This turns out to be
equivalent to what you would get by applying the stochastic gradient
descent algorithm of @robbins1951 to the Bradley-Terry model.

Elo's work also handled ties, which he handled by considering a
``Bernoulli''-like observation of 0.5.  @davidson1970 developed a
properly generative model by generalizing the two-category
Bradley-Terry model to an ordinal logistic regression.  We will return
to the ordinal logistic model later when we combine numerical ratings
and rankings.



# The Plackett-Luce model of ranking

## The Luce axiom

@luce1959 introduced a normative axiom govering choice, namely that if
we are to stochasically choose among $N$ objects according to some
discrete probability distribution, then if we choose among a subset of
the objects, the relative probabilities are the same.  For example, if
suppose I am given a choice among three ice cream flavors, vanilla,
chocolate, and strawberry, and I choose among them with a 70%
proabability of choosing strawberry, a 20% probability of choosing
vanilla, and a 10% probability of choosing chocolate. Then Luce's
axiom states that if I had a choice of only vanilla and chocolate then
my probability of choosing vanilla should be $\frac{0.2}{0.2 + 0.1}$
and if I had a choice between chocolate and strawberry, my probability
of choosing strawberry will be $\frac{0.7}{0.7 + 0.1}.$ Similarly,
let's say I add butter pecan and now have a four way choice.  If I
have a 50% chance of choosing butter pecan, then my chance of choosing
strawberry must be $\frac{0.7}{2}$, my chance of choosing vanilla
$\frac{0.2}{2}$ and my chance of choosing chocolate $\frac{0.1}{2}$.
We just normalize the probabilities so that they add up to 1 and the
relative probabilities among our original choices remain the same.

## Plackett-Luce sampling distribution

@plackett1975 independently discovered the model two decades later and
it has come to be known as the Plackett-Luce model.  The ranking model
is generative.  Given a set of items, they are chosen one by one
without replacement to produce the ranking.  The probabilty of
choosing an item from a set of items is always proportional to the
item's quality.  This will ensure that the likelihood will satisfy
Luce's axiom by construction.

The Plackett-Luce sampling distribution (aka likelihood when viewed as
a function of the parameters) defines the probability of observing
rankings.  We will assume there is a set of $I$ items. The
distribution is parameterized by a positive-constrained $I$-vector
$\beta \in (0, \infty)^I$.

The data we will observe will be ascending rank orderings of $K$
items.  For example, if a rater is given items 13, 48, 9, and 83, an
observation might be $Y = (13, 48, 9, 83).$  The probability of an 
obseration is defined to satisfy the Luce axioms with item probabilities
proportional to $\beta_i$,
$$
\Pr[Y = (13, 48, 9, 83) \mid \beta]
= \frac{\beta_{83}}{\beta_{83} + \beta_9 + \beta_{48} + \beta_{13}}
\cdot \frac{\beta_9}{\beta_9 + \beta_{48} + \beta_{13}}
\cdot \frac{\beta_{48}}{\beta_{48} + \beta_{13}}.
$$
Given a general $K$-array $y \in (1:I)^K$ without duplicated entries,
the probability mass function for the Plackett-Luce model with
positive-constrained parameters is
$$
p(y \mid \beta)
= \prod_{k = 2}^{K} \frac{\beta_{y_k}}{\sum_{k' = 1}^k \beta_{y_{k'}}}.
$$

### Normalization {.unnumbered}

The normalization of this model is a bit subtle.  As specified here,
the model technically conditions on the items being ranked. That is, a
rater is given items and asked to rank them.  Thus we should
technically be writing the generative model as $p(y \mid \beta,
\textrm{set}(y))$ to indicate the conditioning on the set of elements
$\textrm{set}(y) = \{ y_k : k \in 1:K \}$ being ranked.  The items
chosen to have raters rank is part of the experimental design, which
can be as simple as assigning ten items to each rater chosen uniformly
from the set of items without replacement.  But whatever the design
is, it is assumed to be fixed.  An alternative would be to assign data
to rank randomly according to a random design that could be modeled.
For example, we could give each rater $\textrm{Poisson}(5)$ items to
rate chose uniformly from all items without replacement.

The sequential nature of the selection means that a simple inductive
proof is enough to show the distribution is properly normalized given
the items being ranked, so that
$$
\sum_{y' \textrm{ a permuation of } y} p(y' \mid \beta, \textrm{set}(y)) = 1.
$$

### Identification

We have the same problem with multiplicative non-identifiability as we
do in the Bradley-Terry model with positive-constrained parameters and
we will employ the solution we mentioned earlier to identify the
model, which is to constrain the parameters to lie in the $I -
1$-simplex, i.e., $\beta \in \Delta^{I-1}$, where we define
$$
\Delta^{I-1}
= \{ x \in [0, \inf)^I \
     : \ x_i \geq 0 \textrm{ and } \textrm{sum}(x) = 1 \}.
$$

## Extended ranking models

### Top-*n* models

The Plackett-Luce model naturally extends to observations such as
top-10 lists drawn from among a larger set of items.  For example,
choosing the top 10 items among a 100-item set would be modeled by
following the Luce axiom, where $y$ is the top items in ascending
order. 
$$
p(y \mid \beta)
= \prod_{n=1}^{10}
\frac{\beta_{y_n}}
     {\textrm{sum}(\beta) - \beta_{y_1} - \cdots - \beta_{y_{n-1}}}.
$$     
Such a model would be useful in cases where the tail of the ranking is
censored.  For instance, one might call off a race after the first ten
finishers have crossed the finish line.

### *n*-th place finishers

Suppose we are interested in the probablity of a given item occurring
at a given rank given our observed ratings? This turns out to be easy
to calculate using MCMC methods, as we will show later. The formal
definition is a little more daunting.  For example, if we are
interested in the probability of an item being ranked third best
overall, we can take
$$
\Pr[i \textrm{ ranked 3rd}]
= \sum_{j = 1}^I \, \sum_{k = 1}^I \textrm{I}(j \neq i) \, \textrm{I}(j \neq
k) \, \textrm{I}(k \neq i) \, \Pr[\textrm{top 3 are } i, j, k],
$$
where the top-3 probability is calculated as described in the previous
section.

This requires considering $\mathcal{O}(I^2)$ items that might be
higher ranked.   Extending to higher ranks explicitly is going to grow
in cost exponentially, with rank $N$ costing $\mathcal{O}(I^N)$.  It
turns out that estimating these ranking probabilities to within a
given standard error is much more tractable.


### Win, show, or place

The motivation for the developing the model, according to the first
line of @plackett1975, is

> In 1965 I was consulted by a bookmaker with the following problem.
When there are 8 or more runners in the field, a punter may bet that a
specified horse will be placed, i.e. finish in the first three.

The term "punter" is British slang for a persom who bets against a
bookmaker. The term "place" means finishing among the top three and
"show" means finishing in the top two. Consider the probability of
placing, which is just the probability of the mutually exclusive
events of finishing first, second, or third,
$$
\Pr[i \textrm{ places}]
= \Pr[i \textrm{ranked first}]
+ \Pr[i \textrm{ranked second}]
+ \Pr[i \textrm{ranked third}].
$$

With the model we are formulating, we can calculate the
probability of an item being ranked 3rd by marginalizing over all the
first two choices.  This will cost $\mathcal{O}(I^2)$ when there are
$I$ items and lower ranks go up in cost exponentially.  But it can be
defined by
where the top-3 probability is as defined in the previous section.



## Fitting Plackett-Luce with Stan

### Coding the model {.unnumbered}

The Stan code for the model relies on the fact that the denominators
in the sampling probabilty mass function are terms in the cumulative
sum of all of the items being ranked.

```{.stan include="plackett-luce.stan" filename="plackett-luce.stan"}
```

In more detail, we have defined a function `placket_luce_lpmf`, which
allows us to model each observation $y_n$ as beign generated from that
distribution.  The conditioning on the set of items being ranked is
implicit.  We use the elementwise division operator `./` to divide the
parameters by the cumulative sums in the definition.  We then put
everything on the log scale, which turns the product into a sum.

The rankings are themselves an array of integers of size $R \times K$,
which encodes the assumption that each rater ranks the same number of
items.  The constraints on `y` implement a test to make sure the
elements supplied as data are indexes into the items $1:I$.

The parameters are declared as a vector of size `I` that form a
simplex (i.e., a member of $\Delta^{I - 1}$).  The model is ust a
for-each loop, with `y_n in y` iterating over all of the elements of
`y`.  Bcause `y` is declared as a two-dimensional `R` by `K` array,
`y_n` will be a one-dimensional `K` array.


### Fitting the model {.unnumbered}

First, we collect all the data.
```{python}
#| code-fold: true
df = pd.read_csv('sushi3-2016/sushi3b.5000.10.order',
                     delim_whitespace=True, header=None, skiprows=1)
df = df.drop(columns=[0, 1])
y = df.to_numpy()
y = y + 1   # data indexes items from 0
y = y.astype(int)
rev_row_y = y[:, ::-1]  # reverse rows
data_dict = {'I': 100, 'R': 5000, 'K': 10, 'y': rev_row_y }
```

Then we compile the model.

```{python}
#| output: false
model = csp.CmdStanModel(stan_file = 'plackett-luce.stan')
```

The model defines the posterior density, from which we can sample
using the `sample` method.

```{python}
#| output: false
fit = model.sample(data = data_dict, chains = 1,
                   iter_warmup = 30, iter_sampling = 30,
                   parallel_chains = 4)
```

In order to inspect the results, we can read in the metadata
from the supplied file and extract the item names.

```{python}
#| code-fold: true
df_items = pd.read_csv('sushi3-2016/sushi3.idata',
                           delim_whitespace=True, header=None)
item_names = df_items.iloc[:, 1].values
```

The following code reads the English descriptions out of lines 56--155
of the mostly unstructured file `README-en.txt`, where entries look as
follows. 

```
 0:ebi (shrimp)
```

```{python}
#| code-fold: true
desc = []
with open('sushi3-2016/README-en.txt', 'r') as f:
    for i, line in enumerate(f):
        if i < 55: continue
        if i >= 155: break
        paren_start = line.find('(') + 1
        paren_end = line.find(')')
        desc.append(line[paren_start:paren_end])
```

We then define point estimates of the parameters as posterior means.

```{python}
item_scores = [np.mean(fit.stan_variable('beta')[:, i])
               for i in range(100)]
```

Then we can put the item names and scores into a data frame, sort by
value, and print.

```{python}
#| code-fold: true
df_out = pd.DataFrame({'type': item_names, 'score': item_scores, 'desc': desc})
df_out.sort_values(by='score', ascending=False, inplace=True)
print(f"rank  score   type (description)")
for index, (_, row) in enumerate(df_out.iterrows()):
    print(f"{(index + 1):4d}  {row['score']:6.4f}  {row['type']}  ({row['desc']})")
```

The top four pieces are all tuna, followed by two forms of shrimp and
then salmon.  Yellowtail (buri) is in the mid-20s and raw beef
(gyusashi) is the mid-50s.  There are also many pieces you don't find
very often in the United States as sushi, including kujira (whale),
kamo (duck), raw chicken (samami) and blowfish (fugu).  The pieces
with the lowest scores are sea cucumber (namako), okra (okura) and seq
squirt (hoya).


# Ordinal logistic rating model

Now we turn to dealing with the rating data, where each of the raters
provided a score between 0 and 4 (4 being best) for the same 10 types
of sushi as they ranked.  Here, we are going to employ an ordinal
logistic rating model.  The underlying generative model is
straightforward.  We generate scores for each of the items on a log
odds scale, then we set cutoffs for the rating.

## Generative ordinal logistic model

Specifically, suppose we have $K$ categories, which in our case has $K
= 5$ because raters provide ratings on a 1 to 5 scale.   Each item
gets a score $\eta_i \in \mathbb{R}$ and we have a vector of cutpoints
$c \in \mathbb{R}^{K-1}$ that is ordered so that $c_1 < c_2 \cdots <
c_{K-1}$.  The probabilty of providing a rating of $k$ is given by
$$
\textrm{OrderedLogistic}(k \mid \eta, c)
=
\begin{cases}
1 - \textrm{logit}^{-1}(\eta - c_1)
& \textrm{if } k = 1
\\[4pt]
\textrm{logit}^{-1}(\eta - c_{k-1}) - \textrm{logit}^{-1}(\eta - c_k)
& \textrm{if } k > 1 \textrm{ and } k < K
\\[4pt]
\textrm{logit}^{-1}(\eta - c_{K-1}) - 0
& \textrm{if } k = K.
\end{cases}
$$

Because inverse logit is the cumulative distribution function of the
standard logistic distribution, these probabilities have a natural
interpretation.  The cutpoints divide the real line into intervals,
and the probabilities here are the cumulative probability of those
intervals.  The value of $\eta$ shifts the logistic distribution under
the cutpoints.  We can visualize this as follows.  For the sake of
this example, let's suppose we have $K = 4$ and we have cutpoints
$c = [-1 \quad 0 \quad 1]^{\top}$.  We can plot those cutpoints over
the distribution $\textrm{logistic}(\eta, 1)$ to visualize how the
probabilities vary with $\eta$.

```{python}
#| fig-cap: "**Visualizing ordinal logit probabilities.** *The plots show a standard logistic distribution shifted by *`eta`*.  The blue lines indicate the cutpoints, which partition the x-axis into four regions.  The areas of the shifted logistic curves in those regions are the probabilities of the ordinal outcomes.  For example, the probability of the lowest rating of 1 is the area to the left of the leftmost cutpoint, the probability of a 2 outcome the area between the two leftmost cutpoints, and the probability of a 4 outcome is the area to the right of the blue line.  As *`eta`* increases, the probabilities of higher ratings go up.*"
#| code-fold: true

x_values = np.linspace(-8, 10, 200)
categories = [-2, 0, 4]
dfs = []
for cat in categories:
    y_values = sp.stats.logistic.pdf(x_values, loc=cat, scale=1)
    temp_df = pd.DataFrame({'x': x_values, 'y': y_values, 'Category': [f"eta = {cat}"] * len(x_values)})
    dfs.append(temp_df)
final_df = pd.concat(dfs, ignore_index=True)
plot = (pn.ggplot(final_df, pn.aes(x='x', y='y'))
        + pn.geom_line()
        + pn.geom_vline(pn.aes(xintercept=-1.2), color='blue')
        + pn.geom_vline(pn.aes(xintercept=0.3), color='blue')
        + pn.geom_vline(pn.aes(xintercept=2.9), color='blue')
        + pn.scale_x_continuous(limits=(-8, 10), expand=(0, 0))
        + pn.scale_y_continuous(limits=(0, 0.26), expand=(0, 0))
        + pn.labs(x="x", y="logistic(x - eta | 0, 1)")
        + pn.facet_wrap('~Category')
        + pn.theme(panel_spacing=0.025,
	           figure_size = (8, 8/3))
)
print(plot)
```
In this example, we are going to use a single sequence of cutpoints
and assume that the scores do not vary by rater.  Of course, people
have different preferences for sushi, but what we are trying to
estimate is something like a consensus view of preferences.  

## Identifiability

The only terms involving the parameters are of the form $\eta_i - c_k$
where $\eta_i$ is an item quality and $c_k$ is a cutpoint.  This means
that the model is not identified in the sense that
$$
\textrm{OrderedLogistic}(k \mid \eta, c)
= \textrm{OrderedLogistic}(k \mid \eta + d, c + d),
$$
where $d \in \mathbb{R}$.  We can deal with this non-identifiabilty by
constraining the quality parameter vector to sum to zero.  We will do
this by taking free parameters $\eta_1, \ldots, \eta_{I-1}$ and defining
$$
\eta_I = -\sum_{i = 1}^{I - 1} \eta_i.
$$

## Coding ordinal logistic in Stan

Stan has a buil-in distribution `ordered_logistic`, whose probability
density function was defined above.

```{.stan include="ordinal-logit.stan" filename="ordinal-logit.stan"}
```

We code the $K - 1$ cutpoints using Stan's data type `ordered`, which
constrains the vector `c` to have $K - 1$ entries in ascending order
(in the sushi example, $K = 5$ because ratings are on an ordinal 1--5
scale.  

The item quality values are declared as the parameter vector `eta_pre`
(the "pre" being for prefix).  We then construct the entire vector
`eta` as a transformed paraemter by appending the negative sum of the
first $K - 1$ entries as the $I$-th entry.

Both the cutpoints and the quality values are centered at zero and
given weakly informative priors on the logistic scale.  We chose the
scale of 3 for these priors because they are weakly informative on the
scale of the values.  At two standard deviations out from the mean,
we see that we have fairly wide bounds on the inverse logit scale we
are using for the parameters (which is called `expit` in Scipy).

```{python}
q05 = sp.special.expit(-6)
q95 = sp.special.expit(6)
print(f" 5% quantile = inv_logit(-6) = {q05:5.3f}")
print(f"95% quantile = inv_logit( 6) = {q95:5.3f}")
```

## Exploring the constrained prior

Because of the ordering constraint on the cutpoint parameter `c`
and the sum-to-zero constraint on `eta`, the elements are not
independent and their priors will not be $\textrm{normal}(0, 3)$.

Analytical exploration would requiring manipulating multidimensional
changes of variables (see the *Stan Reference Manual* section on
constrained variables from the @stan2023).  Instead, we will explore
the behavior by running a simple Stan model with only the
prior (we could have also run the full model with size zero data).

```{.stan include="ordinal-logit-prior.stan" filename="ordinal-logit-prior.stan"}
```

In both cases, we have defined the prior directly on the constrained
parameter values.


### Sum-to-zero prior

First, let's look at the quality values `eta`, which are constrained to
sum to zero.  With this constraint, there are only $I - 1$ parameters
required to produce an $I$-vector that sums to zero (i.e., we have one
fewer degrees of freedom than the vector size so that the parameter
is of intrinsic dimensionality $I - 1$ in the topological sense).

```{python}
#| output: false
model_ppc = csp.CmdStanModel(stan_file = 'ordinal-logit-prior.stan')
fit_ppc = model_ppc.sample(data = {'I': 8, 'K': 5}, chains = 1,
                           iter_warmup = 500, iter_sampling = 10000)
```

We can then extract the summary (which is a pandas table) to get a
sense of whether the model fit.

```{python}
summary = fit_ppc.summary(percentiles=(5, 95), sig_figs=2)
print(summary)
```

Not surprisingly, the R-hat values are all near 1 and the effective
sample sizes are greater than the number of iterations (as often
arises with Hamiltonian Monte Carlo applied to simple target
densities).  Now let's look directly at all of the eta variables and
their standard deviations.


```{python}
draws = fit_ppc.stan_variable('eta')
for i in range(8):
  print(f"mean(eta[{i + 1}]) = {np.mean(draws[:, i]): 4.2f}   sd(eta[{i + 1}]) = {np.std(draws[:, i]): 3.1f}")
```

This shows the sum-to-zero constraint leaves a symmetric prior on the
elements.  With $I = 8$, we have a noticeable reduction in scale
from 3 to 2.8 (from the standard deviation).  With $I = 100,$ this
reduction is barely noticeable.

It is *not* equivalent to put the prior on `eta_pre`.  In that
situation, the prior is no longer symmetric on `eta`.  The elememts of
`eta` will still be centered around zero, but the prior scale on
$\eta_1$ to $\eta_7$ will be 3 (instead of being reduced) and the
prior variance on $\eta_8$ will be seven times the prior variance of
$\eta_1$ through $\eta_7$ because it's the sum of seven independent
normals (the variance of the sum of a sequence of random variables is
the sum of their variances and sign flip doesn't matter due to
symmetry), so that it's scale will be inflated by a factor of
$\sqrt{7}$.  And clearly the problem gets worse as $I$ increases.


### Ordered cutpoint prior

We have constrained our cutpoint vector `c` to be ordered, which
implies it will be in ascending order, with `c[1]` the smallest
element and `c[K - 1]` the largest. Let's see what those parameter
means and standard deviations look like.

```{python}
draws = fit_ppc.stan_variable('c')
for k in range(4):
  print(f"mean(c[{k + 1}]) = {np.mean(draws[:, k]): 4.2f}   sd(c[{k + 1}]) = {np.std(draws[:, k]): 3.1f}")
```

Like the sum-to-zero case, we know that the elements of the ordered
vector `c` are not independent because of the constraint.  Unlike the
sum-to-zero case, the marginal distribution of each element is not
identical.  The prior mean for $c_1$ is roughly -3 and that of $c_2$
roughly -0.9.  The prior means are in ascending order like the
values.  The standard deviations are higher for the first ($c_1$) and
last ($c_4$) elements, which makes sense because the boundary elements
are only constrained on one side whereas the other elements are
constrained on both sides.

The ordered cutpoint prior is equivalent to what we would get from
taking independent $\textrm{normal}(0, 3)$ draws and sorting them.

```{python}
draws = np.random.normal(loc=0, scale=3, size=(10000, 4))
draws.sort(axis=1)
means = np.mean(draws, axis=0)
sds = np.std(draws, axis=0)
for k in range(4):
  print(f"mean(c[{k + 1}]) = {means[k]: 4.2f}   sd(c[{k + 1}]) = {sds[k]: 3.1f}")
```

In statistics, the sorted values are known as *order statistics* (any
old function of a random variable is called a "statistic").  They have
the same mean and stadard deviation as we get in Stan with a prior on
ordered parameters.

Although the means are quite spread out with this prior, the standard
deviation of each parameter in the prior is quite high.  For example,
the 95% interval of $c_1$ is roughly $(-7, 4)$.


### Preventing category collapse

Given our application as a set of cutpoints, if we wind up with
cutpoints very close together, the category represented by the pair
will have vanishingly small probability.  Let's see what the prior
distribution is on the gaps between cutpoints:

```{python}
#| code-fold: true
import pandas as pd
from plotnine import ggplot, aes, geom_histogram, facet_wrap
diffs = np.diff(draws, axis=1)
df = pd.DataFrame(diffs, columns=['c[2] - c[1]', 'c[3] - c[2]', 'c[4] - c[3]'])
df_melted = df.melt(value_name='difference', var_name='k')
plot = (pn.ggplot(df_melted, pn.aes(x='difference'))
        + pn.geom_histogram(color="white", breaks = np.arange(0, 10.5, 0.5), size=0.1, boundary=0)
        + pn.coord_cartesian(xlim = (0, 10))
        + pn.facet_wrap('~ k', ncol=3)
        + pn.theme(panel_spacing=0.025, figure_size = (8, 8/3)))
print(plot)
```

This is not ideal in that there is substantial probability mass pushed
up against the boundaries and the distribution over the differences is
not identical for the edge and middle differences (like the prior
itself).  This will not be a concern for the sushi data that we have,
which has examples of all categories and thus will avoid category
collapse.  

A *zero-avoiding prior* on the space between the cutpoints such as $c_k
- c_{k - 1} \sim \textrm{lognormal}(0, 1)$ could be used to avoid collapse
where $c_k = c_{k-1}$ (which cannot happen in theory, but can in
practice due to limited precision floating point arithmetic).  The
lognormal has support on positive real numbers and it is 
*zero-avoiding* in the sense that
$$
\lim_{x \rightarrow 0} \textrm{lognormal}(x | 0, \sigma) = 0.
$$
Contrast this with $\textrm{exponential}(x | \lambda),$ which also has
support on positive real numbers, but where the limit
is $\infty$ (i.e., the origin is a pole).

This prior on the width between the cutpoints is incomplete as it
doesn't constrain the location of the variables (it only constrains $K - 2$
degrees of fredom of our ordered $K - 1$ vector).  In other words,
it's improper in the sense that the integral over the space of values
isn't finite.  

The prior on cutpoint distance can be completed in an interpretable
way with an overall prior on the sum of the cutpoints to center them,
such as $\textrm{sum}(c) \sim \textrm{normal}(0, \epsilon)$ for some
suitably small $\epsilon$ (e.g., $\epsilon = 0.005$).  No Jacobian
adjustment is needed because summation is linear and we don't need to
include constant terms for Stan's Hamiltonian Monte Carlo samplers.

In Stan, this collapse-avoiding prior is easy to code as

```{stan}
  c[2:K] - c[1:K-1] ~ lognormal(0, 1);
  sum(c) ~ normal(0, 0.005);
```  

In this case study, we will stick to the simpler order-statistic
prior.



### Prior predctive checks

Had we gone one step further and generated data from simulated priors,
we would have what is known as a *prior predictive check* (see, e.g.,
@gabry2019 or @gelman2020, for examples).  Here, the connection between data and the
parameter values is close enough it feels sufficient to investigate
only the prior.


## Fitting the model

Before fitting the model, we have to extract the data into two
parallel 2-dimensional arrays, one with indexes of the pieces being
rated, and one with the ratings.  

```{python}
#| code-fold: true
indexes_list = []
values_list = []
with open("sushi3-2016/sushi3b.5000.10.score", "r") as f:
    for line in f:
        arr = np.fromstring(line.strip(), dtype=int, sep=' ')
        idx = np.where(arr != -1)[0]
        vals = arr[idx]
        indexes_list.append(idx)
        values_list.append(vals)
sushi_types = np.array(indexes_list) + 1
ratings = np.array(values_list) + 1
```

We can print the first few rows of the data we extracted, which
consist of the sushi types and the ratings.

```{python}
print(f"sushi_types (first three raters):\n{sushi_types[range(3), :]}\n")
print(f"ratings (first three raters):\n{ratings[range(3), :]}")
```

Each row corresponds to a rater, with alignment between the two
arrays.  These tables say the first rater assigned sushi type 1 a
rating of 1 (the lowest), sushi type 3 a rating of 5 (the highest),
sushi type 4 a rating of 3, sushi type 12 a rating of 2, and so on.
The third rater gave sush type 1 a rating of 4, sushi type 2 a rating
of 5 and sushi type 6 a rating of 4, and so on.  This is quite
different than the ratings provided by the first two rateers for sushi
types 1 and 6, and overall the third rater provided higher ratings.

One way to adjust for differences in average rating among raters would
be to adjust the overall position of the logistic curve up or down
based on the average score---we will not try this kind of adjustment
here.  This is one of the advantages of ranked data over rated
data---the rating scale is rarely understood in the same way by all of
the raters. This will, of course, show up in the posterior uncertainty
of the ratings.  If the raters are more consistent, the posteriors
will be sharper (i.e., less variable).

Let's fit the model now that we have the data.

```{python}
model_rating = csp.CmdStanModel(stan_file = 'ordinal-logit.stan')
data_dict = {'I': 100, 'K': 10, 'R': 5000, 'J': 5, 'u': sushi_types, 'z': ratings }
fit_rating = model_rating.sample(data = data_dict, chains = 1, 
                                 iter_warmup = 200, iter_sampling = 200)
print(fit_rating.summary())			   
```

Now let's see what the consensus sushi rating looks like under the
ordinal logistic model.

```{python}
item_scores_rating = [np.mean(fit_rating.stan_variable('eta')[:, i])
                      for i in range(100)]
```

and print them.

```{python}
#| code-fold: true
item_names = df_items.iloc[:, 1].values

desc = []
with open('sushi3-2016/README-en.txt', 'r') as f:
    for i, line in enumerate(f):
        if i < 55: continue
        if i >= 155: break
        paren_start = line.find('(') + 1
        paren_end = line.find(')')
        desc.append(line[paren_start:paren_end])

df_out_rating = pd.DataFrame({'type': item_names, 'score': item_scores_rating, 'desc': desc})
df_out_rating.sort_values(by='score', ascending=False, inplace=True)
print(f"rank  score   type (description)")
for index, (_, row) in enumerate(df_out_rating.iterrows()):
    print(f"{(index + 1):4d}  {row['score']:6.4f}  {row['type']}  ({row['desc']})")
```
